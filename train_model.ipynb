{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.oauth2.credentials import Credentials\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from google.auth.transport.requests import Request\n",
        "import os\n",
        "\n",
        "creds = None\n",
        "token_path = 'token.json'\n",
        "creds_path = 'creds.json'\n",
        "SCOPES = ['https://www.googleapis.com/auth/devstorage.read_only']\n",
        "\n",
        "if os.path.exists(token_path):\n",
        "    creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
        "\n",
        "if not creds or not creds.valid:\n",
        "    if creds and creds.expired and creds.refresh_token:\n",
        "        creds.refresh(Request())\n",
        "    else:\n",
        "        flow = InstalledAppFlow.from_client_secrets_file(\n",
        "            creds_path, SCOPES)\n",
        "        creds = flow.run_local_server(port=0)\n",
        "    \n",
        "    with open(token_path, 'w') as token:\n",
        "        token.write(creds.to_json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "LOUudridRIso",
        "outputId": "8cc345ab-bc83-4d97-f2db-821cf7e48b7f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.v2 as v2\n",
        "from torchvision.transforms.v2 import ToTensor\n",
        "\n",
        "from PIL import Image\n",
        "from copy import copy\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from requests.exceptions import SSLError\n",
        "from retry import retry\n",
        "from io import BytesIO\n",
        "\n",
        "gcs_service = build('storage', 'v1', credentials=creds)\n",
        "\n",
        "class SerengetiBBoxDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, images_path, bboxes_path, classes_path, seasons=None, transform=ToTensor()):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.seasons = seasons\n",
        "\n",
        "        images_df = pd.read_csv(images_path)\n",
        "        \n",
        "        with open(bboxes_path, 'r') as f:\n",
        "            self.bbox_objects = json.load(f)['annotations']\n",
        "        \n",
        "        with open(classes_path, 'r') as f:\n",
        "            class_objects = json.load(f)['categories']\n",
        "        \n",
        "        self.classes = {obj['name'].lower(): obj['id'] for obj in class_objects}\n",
        "        self.images = images_df['image_path_rel'].tolist()\n",
        "        if self.seasons:\n",
        "            self.images = list(filter(self.filter_seasons, self.images)) # filter out seasons\n",
        "        self.labels = []\n",
        "        self.bboxes = [[] for _ in range(len(self.images))]\n",
        "\n",
        "        for species in images_df['question__species'].tolist():\n",
        "            species = species.lower()\n",
        "            if species == 'blank':\n",
        "                species = 'empty'\n",
        "            if species == 'vervetmonkey':\n",
        "                species = 'monkeyvervet'\n",
        "            self.labels.append(self.classes[species])\n",
        "        \n",
        "        image_dict = {filename: i for i, filename in enumerate(self.images)}\n",
        "\n",
        "        for obj in self.bbox_objects:\n",
        "            image = obj['image_id'] + '.JPG'\n",
        "            if image in image_dict:\n",
        "                bbox = obj['bbox']\n",
        "                index = image_dict[image]\n",
        "                self.bboxes[index].append(bbox)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(self.bboxes):\n",
        "            if len(self.bboxes[i]) == 0:\n",
        "                del self.images[i]\n",
        "                del self.bboxes[i]\n",
        "            else:\n",
        "                i += 1\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        # Read Image\n",
        "        path = os.path.join(self.root, self.images[i])\n",
        "        # image = Image.open(path, mode='r')\n",
        "        image = self.request_image_from_gcs(path)\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        # Read objects in this image (bounding boxes, labels)\n",
        "        boxes = torch.FloatTensor(self.bboxes[i])               # (n_objects, 4)\n",
        "        labels = torch.LongTensor([self.labels[i]]*len(boxes))  # (n_objects), all objects are same label\n",
        "\n",
        "        if self.transform:\n",
        "            image, boxes, labels = self.transform(image, boxes, labels)\n",
        "\n",
        "        return image, boxes, labels\n",
        "\n",
        "    @retry(exceptions=SSLError, tries=10, delay=1)\n",
        "    def request_image_from_gcs(self, image_path):\n",
        "        bucket_name = 'public-datasets-lila'\n",
        "        image_bytes = BytesIO()\n",
        "        request = gcs_service.objects().get_media(bucket=bucket_name, object=image_path)\n",
        "        media = MediaIoBaseDownload(image_bytes, request)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            _, done = media.next_chunk()\n",
        "\n",
        "        image_bytes.seek(0)\n",
        "\n",
        "        return Image.open(image_bytes)\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        images = list()\n",
        "        boxes = list()\n",
        "        labels = list()\n",
        "\n",
        "        for b in batch:\n",
        "            images.append(b[0])\n",
        "            boxes.append(b[1])\n",
        "            labels.append(b[2])\n",
        "\n",
        "        images = torch.stack(images, dim=0)\n",
        "\n",
        "        return images, boxes, labels  # tensor (N, 3, x, y), 3 lists of N tensors each\n",
        "\n",
        "    def filter_seasons(self, image):\n",
        "        for season in self.seasons:\n",
        "            if season + '/' in image:\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWieb5bHUaS7"
      },
      "source": [
        "#### Custom Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PoteXRid_iu5"
      },
      "outputs": [],
      "source": [
        "# @title ###### Absolute to Fractional Boxes\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class BBoxToFractional(object):\n",
        "    def __call__(self, sample):\n",
        "        \"\"\"\n",
        "            returns: new bounding box in fractional format (ie cx cy w h)\n",
        "        \"\"\"\n",
        "        image, boxes, labels = sample\n",
        "        fractional_boxes = torch.FloatTensor(len(boxes), 4)\n",
        "        for i, box in enumerate(boxes):\n",
        "            x, y, w, h = box\n",
        "            fractional_boxes[i] = torch.Tensor([\n",
        "                x + (w / 2), \n",
        "                y + (w / 2), \n",
        "                w / 2, \n",
        "                h / 2\n",
        "            ])\n",
        "\n",
        "        return image, fractional_boxes, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v1zHQ3tT9KSn"
      },
      "outputs": [],
      "source": [
        "# @title ##### Random Crop\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class BBoxRandomCrop(object):\n",
        "    def __init__(self, scale, ratio):\n",
        "        assert isinstance(scale, tuple)\n",
        "        self.scale = scale\n",
        "        assert isinstance(ratio, tuple)\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        success = False\n",
        "        attempts = 0\n",
        "        image, boxes, labels = copy(sample)\n",
        "\n",
        "        while not success and attempts < 20:\n",
        "            # Randomly determine the scale and ratio of the crop\n",
        "            scale = random.uniform(self.scale[0], self.scale[1])\n",
        "            ratio = random.uniform(self.ratio[0], self.ratio[1])\n",
        "\n",
        "            # Get the height and width of the input image\n",
        "            h, w = image.height, image.width\n",
        "\n",
        "            # Calculate the maximum crop dimensions\n",
        "            cropped_h_max = h\n",
        "            cropped_w_max = w\n",
        "            if ratio > 0:\n",
        "                cropped_w_max = int(cropped_h_max / ratio)\n",
        "            if cropped_w_max > w:\n",
        "                cropped_w_max = w\n",
        "                cropped_h_max = int(cropped_w_max * ratio)\n",
        "\n",
        "            # Calculate the actual crop dimensions based on the scale\n",
        "            area_ratio = np.sqrt(scale)\n",
        "            cropped_h = int(cropped_h_max * area_ratio)\n",
        "            cropped_w = int(cropped_w_max * area_ratio)\n",
        "\n",
        "            # Randomly determine the top-left coordinates of the crop\n",
        "            top = np.random.randint(0, h - cropped_h) if h > cropped_h else 0\n",
        "            left = np.random.randint(0, w - cropped_w) if w > cropped_w else 0\n",
        "\n",
        "            # Calculate the bottom-right coordinates of the crop\n",
        "            right = min(w, left + cropped_w)\n",
        "            bottom = min(h, top + cropped_h)\n",
        "\n",
        "            # Crop the image and convert to PIL format\n",
        "            image = TF.to_tensor(image)\n",
        "            image = image[:, top: bottom, left: right]\n",
        "            image = TF.to_pil_image(image)\n",
        "\n",
        "            updated_boxes = torch.Tensor(len(boxes), 4)\n",
        "            # If the center is off screen, set it to empty\n",
        "            for i, box in enumerate(boxes):\n",
        "                updated_boxes[i] = self.crop_bbox(box, (top, left))\n",
        "\n",
        "            updated_labels = labels\n",
        "            for i, (x,y,_,_) in enumerate(boxes):\n",
        "                if x <= 0 or y <= 0 or x >= cropped_w or y >= cropped_h:\n",
        "                    updated_labels[i] = 0\n",
        "\n",
        "            updated_boxes = updated_boxes[updated_labels != 0]\n",
        "            updated_labels = updated_labels[updated_labels != 0]\n",
        "            \n",
        "            if len(updated_boxes) > 0:\n",
        "                success = True\n",
        "                labels = updated_labels\n",
        "                boxes = updated_boxes\n",
        "            else:\n",
        "                image, boxes, labels = copy(sample)\n",
        "                attempts += 1\n",
        "\n",
        "        return image, boxes, labels\n",
        "    \n",
        "    def crop_bbox(self, bbox, crop):\n",
        "        top, left = crop\n",
        "        x, y, w, h = copy(bbox)\n",
        "        x = bbox[0] - left\n",
        "        y = bbox[1] - top\n",
        "        return torch.Tensor([x,y,w,h])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TOTS4eg3wrT_"
      },
      "outputs": [],
      "source": [
        "# @title ##### Random Horizontal Flip\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class BBoxRandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        assert isinstance(p, float)\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, labels = sample\n",
        "        flip = random.random() <= self.p\n",
        "        if flip:\n",
        "            image = TF.hflip(image)\n",
        "            bboxes = [self.bbox_hflip(image.width, bbox) for bbox in bboxes]\n",
        "            bboxes = torch.stack(bboxes)\n",
        "        return image, bboxes, labels\n",
        "\n",
        "\n",
        "    def bbox_hflip(self, width, bbox):\n",
        "        bbox[0] = width - bbox[0]\n",
        "        return bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VKKcGz9qQjpa"
      },
      "outputs": [],
      "source": [
        "# @title ##### Resize\n",
        "\n",
        "class BBoxResize(object):\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, bboxes, labels = sample\n",
        "        h, w = image.height, image.width\n",
        "        new_h, new_w = self.output_size\n",
        "        image = transforms.Resize((new_h, new_w))(image)\n",
        "        for i, bbox in enumerate(bboxes):\n",
        "            x_scale = new_w / w\n",
        "            y_scale = new_h / h\n",
        "            bboxes[i][0] *= x_scale\n",
        "            bboxes[i][1] *= y_scale\n",
        "            bboxes[i][2] *= x_scale\n",
        "            bboxes[i][3] *= y_scale\n",
        "        return image, bboxes, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYoe_XCf1fwb"
      },
      "source": [
        "#### DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY8suSLUXAFM"
      },
      "source": [
        "##### WRS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y_-n_iK2pca"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch import from_numpy\n",
        "from collections import Counter\n",
        "from torch.utils.data import WeightedRandomSampler, DataLoader, random_split\n",
        "\n",
        "def get_train_loader(train_data, batch_size):\n",
        "    sample_weights = get_sample_weights(train_data)\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train_data), replacement=True)\n",
        "    loader = DataLoader(train_data, batch_size=batch_size, sampler=sampler, num_workers=1)\n",
        "    return loader\n",
        "\n",
        "def get_sample_weights(train_data):\n",
        "    train_indices = train_data.indices\n",
        "    train_classes = np.array(train_data.dataset.targets)[train_indices]\n",
        "\n",
        "    train_class_counts = Counter(train_classes)\n",
        "    class_weights = Counter(train_class_counts)\n",
        "    sample_weights = np.array([1/class_weights[t] for t in train_classes])\n",
        "    return  from_numpy(sample_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRt1aFKAh4Mv"
      },
      "source": [
        "## Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWzcoCm-1BG0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import torchvision.models.detection as detection\n",
        "import torchvision.models.feature_extraction as feature_extraction\n",
        "import numpy as np\n",
        "from copy import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1x_wQGPMU_k"
      },
      "source": [
        "### Base Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfZkldn5h5NW"
      },
      "outputs": [],
      "source": [
        "class Base(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Base, self).__init__()\n",
        "        efficientnet_b3 = torchvision.models.efficientnet_b3(weights='EfficientNet_B3_Weights.DEFAULT')\n",
        "        return_nodes = {\n",
        "            'features.4.0.block.0': \"features_38\",\n",
        "            'features.6.0.block.0': \"features_19\",\n",
        "            'features.8':           \"features_10\",\n",
        "        }\n",
        "        self.feature_extractor = feature_extraction.create_feature_extractor(efficientnet_b3, return_nodes=return_nodes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        features = self.feature_extractor(input)\n",
        "        features_38 = features['features_38']\n",
        "        features_19 = features['features_19']\n",
        "        features_10 = features['features_10']\n",
        "\n",
        "        return features_38, features_19, features_10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFP9ovxfFb94"
      },
      "source": [
        "### Auxiliary Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCPzmzXmFhvk"
      },
      "outputs": [],
      "source": [
        "class AuxiliaryNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryNetwork, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1_1 = nn.Conv2d(1536, 384, kernel_size=1, padding=0)\n",
        "        self.conv1_2 = nn.Conv2d(384, 768, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(768, 192, kernel_size=1, padding=0)\n",
        "        self.conv2_2 = nn.Conv2d(192, 384, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(384, 192, kernel_size=1, padding=0)\n",
        "        self.conv3_2 = nn.Conv2d(192, 384, kernel_size=3, padding=0)\n",
        "        \n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "        \n",
        "\n",
        "    def forward(self, features_10):\n",
        "        # (N, 1535, 10, 10)\n",
        "        out = self.relu(self.conv1_1(features_10))  # (N, 384, 5, 5)\n",
        "        out = self.relu(self.conv1_2(out))  # (N, 768, 5, 5)\n",
        "        features_5 = out  # (N, 768, 5, 5)\n",
        "       \n",
        "        out = self.relu(self.conv2_1(features_5))  # (N, 192, 3, 3)\n",
        "        out = self.relu(self.conv2_2(out))  # (N, 384, 3, 3)\n",
        "        features_3 = out  # (N, 384, 3, 3)\n",
        "       \n",
        "        out = self.relu(self.conv3_1(features_3))  # (N, 192, 1, 1)\n",
        "        out = self.relu(self.conv3_2(out))  # (N, 384, 1, 1)\n",
        "        features_1 = out  # (N, 384, 1, 1))\n",
        "\n",
        "        return features_5, features_3, features_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urhit3i8MMh2"
      },
      "source": [
        "### Prediction Convolutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Uz_0pBMReZ"
      },
      "outputs": [],
      "source": [
        "class PredictionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutions to predict class scores and bounding boxes using lower and higher-level feature maps.\n",
        "    The bounding boxes (locations) are predicted as encoded offsets w.r.t each of the 8732 prior (default) boxes.\n",
        "    See 'cxcy_to_gcxgcy' in utils.py for the encoding definition.\n",
        "    The class scores represent the scores of each object class in each of the 8732 bounding boxes located.\n",
        "    A high score for 'background' = no object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        \"\"\"\n",
        "        :param n_classes: number of different types of objects\n",
        "        \"\"\"\n",
        "        super(PredictionNetwork, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Number of prior-boxes we are considering per position in each feature map\n",
        "        n_boxes = {'features_38': 4,\n",
        "                   'features_19': 6,\n",
        "                   'features_10': 6,\n",
        "                   'features_5': 6,\n",
        "                   'features_3': 4,\n",
        "                   'features_1': 4}\n",
        "        # 4 prior-boxes implies we use 4 different aspect ratios, etc.\n",
        "\n",
        "        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
        "        self.loc_38 = nn.Conv2d(288, n_boxes['features_38'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_19 = nn.Conv2d(816, n_boxes['features_19'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_10 = nn.Conv2d(1536, n_boxes['features_10'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_5 = nn.Conv2d(768, n_boxes['features_5'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_3 = nn.Conv2d(384, n_boxes['features_3'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_1 = nn.Conv2d(384, n_boxes['features_1'] * 4, kernel_size=3, padding=1)\n",
        "\n",
        "        # Class prediction convolutions (predict classes in localization boxes)\n",
        "        self.cl_38 = nn.Conv2d(288, n_boxes['features_38'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_19 = nn.Conv2d(816, n_boxes['features_19'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_10 = nn.Conv2d(1536, n_boxes['features_10'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_5 = nn.Conv2d(768, n_boxes['features_5'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_3 = nn.Conv2d(384, n_boxes['features_3'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_1 = nn.Conv2d(384, n_boxes['features_1'] * n_classes, kernel_size=3, padding=1)\n",
        "                \n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "\n",
        "    def forward(self, features):\n",
        "        batch_size = features[0].size(0)\n",
        "\n",
        "        localization_convolutions = [\n",
        "            self.loc_38,\n",
        "            self.loc_19,\n",
        "            self.loc_10,\n",
        "            self.loc_5,\n",
        "            self.loc_3,\n",
        "            self.loc_1,\n",
        "        ]\n",
        "\n",
        "        class_convolutions = [\n",
        "            self.cl_38,\n",
        "            self.cl_19,\n",
        "            self.cl_10,\n",
        "            self.cl_5,\n",
        "            self.cl_3,\n",
        "            self.cl_1,\n",
        "        ]\n",
        "\n",
        "        localization_predictions = []\n",
        "        class_predictions = []\n",
        "        for l_conv, c_conv, feat in zip(localization_convolutions, class_convolutions, features):\n",
        "            localization_prediction = l_conv(feat)\n",
        "            localization_prediction = localization_prediction.permute(0, 2, 3, 1).contiguous()\n",
        "            localization_prediction = localization_prediction.view(batch_size, -1, 4)\n",
        "            localization_predictions.append(localization_prediction)\n",
        "\n",
        "            class_prediction = c_conv(feat)\n",
        "            class_prediction = class_prediction.permute(0, 2, 3, 1).contiguous()\n",
        "            class_prediction = class_prediction.view(batch_size, -1, self.n_classes)\n",
        "            class_predictions.append(class_prediction)\n",
        "\n",
        "        localizations = torch.cat(localization_predictions, dim=1)\n",
        "        class_scores = torch.cat(class_predictions, dim=1)\n",
        "        return localizations, class_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19AMODIxrvJb"
      },
      "source": [
        "### Bounding Box Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg9C5VcnrxlH"
      },
      "outputs": [],
      "source": [
        "def find_jaccard_overlap(set_1, set_2):\n",
        "    # Find intersections\n",
        "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
        "\n",
        "    # Find areas of each box in both sets\n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
        "\n",
        "    # Find the union\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
        "\n",
        "    return intersection / union  # (n1, n2)#\n",
        "\n",
        "def find_intersection(set_1, set_2):\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
        "\n",
        "def boundary_to_center(xy): #xmin, ymin, xmax, ymax -> centerx, centery, width, height\n",
        "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
        "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
        "\n",
        "def center_to_boundary(cxcy):\n",
        "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n",
        "                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n",
        "\n",
        "\n",
        "def center_to_offsets(cxcy, anchors_cxcy): #cxcy width and height are negative\n",
        "    return torch.cat([(cxcy[:, :2] - anchors_cxcy[:, :2]) / (anchors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
        "                      torch.log(cxcy[:, 2:] / anchors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
        "\n",
        "def offsets_to_center(gcxgcy, anchors_cxcy):\n",
        "    return torch.cat([gcxgcy[:, :2] * anchors_cxcy[:, 2:] / 10 + anchors_cxcy[:, :2],  # c_x, c_y\n",
        "                      torch.exp(gcxgcy[:, 2:] / 5) * anchors_cxcy[:, 2:]], 1)  # w, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52TmIse2g8Yk"
      },
      "source": [
        "### Detection Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BwJl4TDg-8h"
      },
      "outputs": [],
      "source": [
        "class SSD300(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SSD300, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.base = Base()\n",
        "        self.aux = AuxiliaryNetwork()\n",
        "        self.pred = PredictionNetwork(n_classes)\n",
        "\n",
        "        # Since lower level features (features_38) have considerably larger scales, we take the L2 norm and rescale\n",
        "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 288, 1, 1))  # there are 288 channels in features_38\n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "        # Prior boxes\n",
        "        self.anchor_boxes = self.get_anchor_boxes()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        features_38, features_19, features_10 = self.base(batch)\n",
        "\n",
        "        # Rescale conv4_3 after L2 norm\n",
        "        norm = features_38.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
        "        features_38 = features_38 / norm  # (N, 512, 38, 38)\n",
        "        features_38 = features_38 * self.rescale_factors  # (N, 512, 38, 38)\n",
        "\n",
        "        # Run auxiliary convolutions (higher level feature map generators)\n",
        "        features_5, features_3, features_1 = self.aux(features_10) \n",
        "\n",
        "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
        "        locs, class_scores = self.pred([features_38, features_19, features_10, features_5, features_3, features_1])  \n",
        "        \n",
        "        # (N, 8732, 4), (N, 8732, n_classes)\n",
        "        return locs, class_scores\n",
        "    \n",
        "    def get_anchor_boxes(self):\n",
        "        resolutions = [38, 19, 10, 5, 3, 1]\n",
        "        scales = [0.1, 0.2, 0.375, 0.55, 0.725, 0.9]\n",
        "        aspect_ratios = [\n",
        "            [1, 2, 1/2],\n",
        "            [1, 2, 1/2, 3, 1/3],\n",
        "            [1, 2, 1/2, 3, 1/3],\n",
        "            [1, 2, 1/2, 3, 1/3],\n",
        "            [1, 2, 1/2],\n",
        "            [1, 2, 1/2],\n",
        "        ]\n",
        "        \n",
        "        anchor_boxes = []\n",
        "        for k, (resolution, scale, ratios) in enumerate(zip(resolutions, scales, aspect_ratios)):\n",
        "            for i in range(resolution):\n",
        "                for j in range(resolution):\n",
        "                    x = (i + 0.5) / resolution\n",
        "                    y = (j + 0.5) / resolution\n",
        "                    for ratio in ratios:\n",
        "                        w = scale * np.sqrt(ratio)\n",
        "                        h = scale / np.sqrt(ratio)\n",
        "                        anchor_boxes.append([x, y, w, h])\n",
        "                        if ratio == 1 : # add in an additional scaled up square box\n",
        "                            extra_scale = 1.\n",
        "                            if k + 1 < len(scales):\n",
        "                                extra_scale = np.sqrt(scale * scales[k + 1])\n",
        "                            anchor_boxes.append([x, y, extra_scale, extra_scale])\n",
        "\n",
        "        anchor_boxes = torch.FloatTensor(anchor_boxes).to(device)  # (8732, 4)\n",
        "        anchor_boxes.clamp_(0, 1)  # (8732, 4)\n",
        "        return anchor_boxes\n",
        "\n",
        "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_anchors = self.anchor_boxes.size(0)\n",
        "        predicted_scores = nn.functional.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n",
        "\n",
        "        all_images_boxes = list()\n",
        "        all_images_labels = list()\n",
        "        all_images_scores = list()\n",
        "\n",
        "        assert n_anchors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            decoded_locs = center_to_boundary(\n",
        "                offsets_to_center(predicted_locs[i], self.anchor_boxes))  # (8732, 4), these are fractional pt. coordinates\n",
        "\n",
        "            image_boxes = list()\n",
        "            image_labels = list()\n",
        "            image_scores = list()\n",
        "\n",
        "            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n",
        "\n",
        "            for c in range(1, self.n_classes):\n",
        "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
        "                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n",
        "                n_above_min_score = score_above_min_score.sum().item()\n",
        "                if n_above_min_score == 0:\n",
        "                    continue\n",
        "                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n",
        "                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n",
        "\n",
        "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
        "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
        "\n",
        "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)\n",
        "                \n",
        "                # Non-Maximum Suppression (NMS)\n",
        "\n",
        "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
        "\n",
        "                for box in range(class_decoded_locs.size(0)):\n",
        "                    if suppress[box] == 1:\n",
        "                        continue\n",
        "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
        "                    suppress[box] = 0\n",
        "\n",
        "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
        "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
        "                image_scores.append(class_scores[1 - suppress])\n",
        "\n",
        "            if len(image_boxes) == 0:\n",
        "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
        "                image_labels.append(torch.LongTensor([0]).to(device))\n",
        "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
        "\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
        "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
        "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
        "            n_objects = image_scores.size(0)\n",
        "\n",
        "            if n_objects > top_k:\n",
        "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
        "                image_scores = image_scores[:top_k]  # (top_k)\n",
        "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
        "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
        "\n",
        "            all_images_boxes.append(image_boxes)\n",
        "            all_images_labels.append(image_labels)\n",
        "            all_images_scores.append(image_scores)\n",
        "\n",
        "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0E1y7Mx9j3P"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avt37yIZ9lQ8"
      },
      "outputs": [],
      "source": [
        "class MultiBoxLoss(nn.Module):\n",
        "    def __init__(self, anchor_boxes, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.anchor_boxes = anchor_boxes\n",
        "        self.anchor_box_boundaries = center_to_boundary(self.anchor_boxes)\n",
        "        self.threshold = threshold\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.smooth_l1 = nn.L1Loss()\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def get_IoUs(self, boxes):\n",
        "        intersections = torch.FloatTensor(len(boxes), len(self.anchor_boxes))\n",
        "        boxes = boxes/300\n",
        "        for b in range(len(boxes)):\n",
        "            for a in range(len(self.anchor_boxes)):\n",
        "                intersections[b][a] = get_intersection_over_union(box, self.anchor_boxes[a])\n",
        "        return intersections\n",
        "\n",
        "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_anchors = self.anchor_boxes.size(0)\n",
        "        n_classes = predicted_scores.size(2)\n",
        "\n",
        "        assert n_anchors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        true_locs = torch.zeros((batch_size, n_anchors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
        "        true_classes = torch.zeros((batch_size, n_anchors), dtype=torch.long).to(device)  # (N, 8732)\n",
        "        \n",
        "        # For each set of boxes belonging to an image\n",
        "        for i in range(batch_size):\n",
        "            scaled_boxes = torch.stack([box/300 for box in boxes[i]])\n",
        "            n_objects = boxes[i].size(0)\n",
        "            # overlap is a (objects, anchor_boxes) tensor containing the IoU of each anchor for each object\n",
        "            # IoU = self.get_IoUs(boxes[i])\n",
        "            IoU = find_jaccard_overlap(scaled_boxes, self.anchor_boxes)\n",
        "            IoU_for_each_anchor, object_for_each_anchor = IoU.max(dim=0)  # (8732)\n",
        "\n",
        "            _, anchor_for_each_object = IoU.max(dim=1)  # (N_o)\n",
        "\n",
        "            object_for_each_anchor[anchor_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
        "\n",
        "            IoU_for_each_anchor[anchor_for_each_object] = 1.\n",
        "\n",
        "            label_for_each_anchor = labels[i][object_for_each_anchor]  # (8732)\n",
        "            label_for_each_anchor[IoU_for_each_anchor < self.threshold] = 0  # (8732)\n",
        "\n",
        "            true_classes[i] = label_for_each_anchor\n",
        "\n",
        "            true_locs[i] = center_to_offsets(boxes[i][object_for_each_anchor], self.anchor_boxes)  # (8732, 4)\n",
        "        positive_anchors = true_classes != 0  # (N, 8732)\n",
        "\n",
        "        # LOCALIZATION LOSS\n",
        "        loc_loss = self.smooth_l1(predicted_locs[positive_anchors], true_locs[positive_anchors])  # (), scalar\n",
        "\n",
        "        # CONFIDENCE LOSS\n",
        "\n",
        "        n_positives = positive_anchors.sum(dim=1)  # (N)\n",
        "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
        "\n",
        "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
        "        conf_loss_all = conf_loss_all.view(batch_size, n_anchors)  # (N, 8732)\n",
        "\n",
        "        conf_loss_pos = conf_loss_all[positive_anchors]  # (sum(n_positives))\n",
        "\n",
        "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
        "        conf_loss_neg[positive_anchors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
        "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
        "        hardness_ranks = torch.LongTensor(range(n_anchors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
        "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
        "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
        "\n",
        "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
        "        # TOTAL LOSS\n",
        "\n",
        "        return conf_loss + self.alpha * loc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Q4Zbxr9o6t"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIigYLKXz1pg",
        "outputId": "7d8fd25d-2374-44b2-c933-13f3387d58fd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4caOV_eNNzno"
      },
      "outputs": [],
      "source": [
        "# @title Epoch\n",
        "from datetime import datetime\n",
        "\n",
        "def free_up_memory():\n",
        "    keys = ['images', 'labels', 'boxes', 'predicted_scores', 'predicted_scores']\n",
        "    for key in keys:\n",
        "        if key in globals().keys():\n",
        "            del globals()[key] \n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()  # training mode enables dropout\n",
        "    epoch_loss = AverageMeter()\n",
        "    epoch_time = AverageMeter()\n",
        "    # Batches\n",
        "    print(f'Epoch {epoch}')\n",
        "    for i, (images, boxes, labels) in enumerate(train_loader):\n",
        "        start = datetime.now()\n",
        "        # Move to default device\n",
        "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
        "        boxes = [b.to(device) for b in boxes]\n",
        "        labels = [l.to(device) for l in labels]\n",
        "\n",
        "        # Forward pass\n",
        "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
        "        \n",
        "        # Backward prop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients, if necessary\n",
        "\n",
        "        # Update model\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss.update(loss.item())\n",
        "        epoch_time.update((datetime.now() - start).seconds)\n",
        "        # Print status\n",
        "        if i % 10 == 0:\n",
        "            print(f'Epoch {epoch}\\t Batch {i} / {len(train_loader)}\\tBatch Loss:\\t {epoch_loss.val}')\n",
        "    free_up_memory()  # free some memory since their histories may be stored\n",
        "    print(f'Avg Loss:\\t {epoch_loss.avg}')\n",
        "    print(f'Avg time:\\t {epoch_time.avg}')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAQ5Izvp9oL_"
      },
      "outputs": [],
      "source": [
        "# @title ### Training Loop\n",
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "\n",
        "def train_model(train_dataset, checkpoint= None):\n",
        "    # Model parameters\n",
        "    n_classes = len(train_dataset.dataset.classes)\n",
        "\n",
        "    # Learning parameters\n",
        "    batch_size = 32\n",
        "    num_workers = 16\n",
        "    iterations = 120000  # number of iterations to train\n",
        "    lr = 1e-3                       # learning rate\n",
        "    decay_lr_at = [80000, 100000]   # decay learning rate after these many iterations\n",
        "    decay_lr_to = 0.5   # decay learning rate to this fraction of the existing learning rate\n",
        "    momentum = 0.9      # momentum\n",
        "    weight_decay = 5e-4 # weight decay\n",
        "    grad_clip = None    # clip if gradients are exploding, which may happen at larger batch sizes \n",
        "                        # (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n",
        "    epoch = 0\n",
        "\n",
        "    # Initialize model or load checkpoint\n",
        "    if checkpoint == None:\n",
        "        model = SSD300(n_classes=n_classes)\n",
        "        biases = list()\n",
        "        not_biases = list()\n",
        "        for param_name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if param_name.endswith('.bias'):\n",
        "                    biases.append(param)\n",
        "                else:\n",
        "                    not_biases.append(param)\n",
        "        optimizer = torch.optim.SGD(lr=lr, \n",
        "                                    momentum=momentum, \n",
        "                                    weight_decay=weight_decay,\n",
        "                                    params=[{'params': biases, 'lr': 2 * lr},{'params': not_biases}])\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "        epoch = checkpoint['epoch'] + 1\n",
        "        print('\\nLoaded checkpoint from epoch %d.\\n' % epoch)\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "\n",
        "    # Move to default device\n",
        "    model = model.to(device)\n",
        "    criterion = MultiBoxLoss(anchor_boxes=model.anchor_boxes).to(device)\n",
        "\n",
        "    # Setup data loader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=batch_size, \n",
        "                                               num_workers=num_workers, \n",
        "                                               collate_fn=train_dataset.dataset.collate_fn, \n",
        "                                               shuffle=True)\n",
        "\n",
        "    epochs = iterations // (len(train_dataset) // batch_size)\n",
        "    decay_lr_at = [it // (len(train_dataset) // batch_size) for it in decay_lr_at]\n",
        "    decay_lr_scale = 0.7\n",
        "\n",
        "    for epoch in range(epoch, epochs):\n",
        "        # Decay learning rate at specified points\n",
        "        if epoch in decay_lr_at:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = param_group['lr'] * decay_lr_scale\n",
        "\n",
        "        # Train model for an epoch\n",
        "        model = train(train_loader=train_loader,\n",
        "                      model=model,\n",
        "                      criterion=criterion,\n",
        "                      optimizer=optimizer,\n",
        "                      epoch=epoch)\n",
        "        \n",
        "        # Save the model to a file\n",
        "        state = {'epoch': epoch,\n",
        "                 'model': model,\n",
        "                 'optimizer': optimizer}\n",
        "        filename = 'trained_model.pth.tar'\n",
        "        torch.save(state, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE5sxVzcKlmP"
      },
      "outputs": [],
      "source": [
        "# @title ### Get datasets\n",
        "def get_train_val_datasets(seasons=None):\n",
        "    # File Paths\n",
        "    root = 'snapshotserengeti-unzipped/'\n",
        "    annotations_directory = \"./dataset\"\n",
        "    images_path = annotations_directory + \"/SS_bbox_images.csv\"\n",
        "    classes_path = annotations_directory + \"/SS_Label_Classes.json\"\n",
        "    boxes_path = annotations_directory + \"/SS_BBoxes.json\"\n",
        "\n",
        "    # Whole dataset\n",
        "    dataset = SerengetiBBoxDataset(root, images_path, boxes_path, classes_path, seasons=seasons)\n",
        "\n",
        "    # Split data into train and validation sets\n",
        "    train_split = 0.7\n",
        "    n_train = int(len(dataset) * train_split)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, (n_train, len(dataset)-n_train),)\n",
        "    \n",
        "    # Train transform\n",
        "    train_transform = v2.Compose([\n",
        "        BBoxToFractional(),\n",
        "        BBoxRandomHorizontalFlip(),\n",
        "        BBoxRandomCrop((0.7,1.0), (0.9,1.1)),\n",
        "        BBoxResize((300, 300)),\n",
        "        v2.ColorJitter(brightness=0.1, contrast=0.05),\n",
        "        v2.Compose([v2.ToImageTensor(), v2.ConvertImageDtype()]),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = v2.Compose([\n",
        "        BBoxToFractional(),\n",
        "        BBoxResize((300, 300)),\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Apply train transformations\n",
        "    train_dataset.dataset = copy(dataset)\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "    val_dataset.dataset.transform = val_transform\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "def show_image(image, boxes=None):\n",
        "    # image = val_data[idx][0]\n",
        "\n",
        "    display_transformation = transforms.Compose([\n",
        "        transforms.Normalize(mean = [ 0., 0., 0. ], \n",
        "        std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
        "        transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
        "        std = [ 1., 1., 1. ]),\n",
        "        v2.ToPILImage(),\n",
        "        ])\n",
        "\n",
        "    image = display_transformation(image)\n",
        "    print(image)\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(image)\n",
        "\n",
        "    if boxes:\n",
        "        # # Create a Rectangle patch\n",
        "        img_boxes = list(boxes[0][0])\n",
        "        box = [i.item() for i in img_boxes]\n",
        "        print(box)\n",
        "        x, y, w, h = box\n",
        "\n",
        "        rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
        "\n",
        "        # # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkw6bx9eP2hB",
        "outputId": "9fdb5c55-475f-4a87-8fa3-7f15e4d1b320"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "train_data, val_data = get_train_val_datasets(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4gpiTOT3YI4",
        "outputId": "fe6fec91-181e-4433-e214-ad28b56a4671"
      },
      "outputs": [],
      "source": [
        "# show_image(train_data.dataset[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "kxw5DCJOvgdI",
        "outputId": "4e38c171-f78f-478d-e595-a02b3af7fc6c"
      },
      "outputs": [],
      "source": [
        "# train_model(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIbBYHuAFr7Z",
        "outputId": "b0b8b450-6409-4aa1-8fc5-4c977bce54b4"
      },
      "outputs": [],
      "source": [
        "# idx = random.randint(0, 200)\n",
        "def validate(val_data, checkpoint, idx):\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model = checkpoint['model']\n",
        "    batch = val_data[inx][0].unsqueeze(0)\n",
        "    predicted_locs, predicted_scores = model.forward(batch)\n",
        "    return model.detect_objects(predicted_locs, \n",
        "                         predicted_scores, \n",
        "                         min_score=0.01, \n",
        "                         max_overlap=0.45, \n",
        "                         top_k=200)    \n",
        "    \n",
        "# boxes, labels, scores = validate(val_data, '/content/trained_model.pth.tar', idx)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "CC0o_w3jj4rX",
        "g7EvPhITUqx_",
        "BsPK3nPT5n-X",
        "XWieb5bHUaS7",
        "kYoe_XCf1fwb",
        "bY8suSLUXAFM",
        "rRt1aFKAh4Mv",
        "X1x_wQGPMU_k",
        "lFP9ovxfFb94",
        "urhit3i8MMh2",
        "19AMODIxrvJb",
        "52TmIse2g8Yk",
        "i0E1y7Mx9j3P",
        "VPFWNvGLFo1C"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
